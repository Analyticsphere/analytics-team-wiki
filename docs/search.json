[
  {
    "objectID": "qaqc.html",
    "href": "qaqc.html",
    "title": "QAQC",
    "section": "",
    "text": "Here’s some documentation"
  },
  {
    "objectID": "optimizing-data-handling.html",
    "href": "optimizing-data-handling.html",
    "title": "Optimizing Data Handling with R and BigQuery",
    "section": "",
    "text": "THIS TUTORIAL IS CURRENTLY UNDER DEVELOPMENT. THIS NOTE WILL BE REMOVED WHEN FINALIZED\nTODO: (1) Make code chunks executable (2) Use functioning query related to a more relevant example"
  },
  {
    "objectID": "optimizing-data-handling.html#introduction",
    "href": "optimizing-data-handling.html#introduction",
    "title": "Optimizing Data Handling with R and BigQuery",
    "section": "Introduction",
    "text": "Introduction\nAs our data set continues to grow, optimizing data handling becomes paramount for computational efficiency and memory usage. This tutorial explores four methods to achieve these goals, focusing on offloading computation to BigQuery through SQL queries. By following simple rules, such as being selective about data loading and leveraging Materialized Views, you can streamline your data workflows and make them more efficient."
  },
  {
    "objectID": "optimizing-data-handling.html#simple-rules-for-optimization",
    "href": "optimizing-data-handling.html#simple-rules-for-optimization",
    "title": "Optimizing Data Handling with R and BigQuery",
    "section": "Simple Rules for Optimization",
    "text": "Simple Rules for Optimization\nTo ensure efficient data handling, let’s agree on some simple rules:\n\nBe Selective: Load only the variables you need.\nLeverage BigQuery: Perform filters, joins, and manipulations in BigQuery using SQL or dbConnect/dbplyr.\nUse Views: Consider creating Materialized Views in BigQuery for repetitive operations."
  },
  {
    "objectID": "optimizing-data-handling.html#methods-of-loading-and-processing-data-from-bigquery-in-r",
    "href": "optimizing-data-handling.html#methods-of-loading-and-processing-data-from-bigquery-in-r",
    "title": "Optimizing Data Handling with R and BigQuery",
    "section": "4 Methods of Loading and Processing Data from BigQuery in R",
    "text": "4 Methods of Loading and Processing Data from BigQuery in R\nThis tutorial explores four distinct methods for efficiently loading and processing data from BigQuery in the R programming language. Handling large datasets and performing data transformations is a fundamental aspect of data science and analytics. As data scientists and engineers, it’s crucial to adopt practices that optimize computational efficiency, memory usage, and maintainability in our data workflows.\nIn this tutorial, we delve into four distinct approaches, each with its own set of advantages and considerations. We’ll perform the same filters, joins and calculations in each example, but accomplish them in different ways. Then we’ll discuss the trade-offs of each method.\n\nLoad dependencies & parameterize\nlibrary(bigrquery)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(dplyr)\n\n# Define project, dataset, and table IDs\nproject_id &lt;- \"bigquery-public-data\"\ndataset    &lt;- \"samples\"\ntable      &lt;- \"natality\"\n\n\nMethod 1: Using bq_table_download\nIn Method 1, we load data using bq_table_download in BigQuery, then perform data filtering, transformations, and joins in the local R environment.\nmethod1_function &lt;- function(project_id, dataset, table) {\n  # Build the SQL query using glue\n  sql_query &lt;- glue::glue(\"SELECT * FROM `{project_id}.{dataset}.{table}` WHERE is_male = 1\")\n  \n  # Execute the query\n  query_result &lt;- bigrquery::bq_query(sql_query)\n  \n  # Fetch and process the data\n  result &lt;- bigrquery::bq_table_download(query_result) %&gt;%\n    dplyr::filter(result, is_male == 1) %&gt;%\n    dplyr::inner_join(result, result, by = \"source\") %&gt;% \n    dplyr::mutate(result, birth_weight_kg = weight_pounds * 0.453592)\n  return(result)\n}\n\ndf1 &lt;- method1_function(project_id, dataset, table)\nhead(df1)\nPros\n\nSimplicity: This method is straightforward and easy to use, making it suitable for quick data retrieval.\nIntuitive Data Manipulation: Data manipulation in R after downloading is more intuitive, allowing for easy filtering, joining, and transformation using familiar R functions.\n\nCons\n\nData Volume Limitation: May not be suitable for very large datasets as it loads the entire table into memory, potentially causing memory issues.\nAdditional Code for Transformation: Requires additional R code for filtering, joining, and transformation after downloading.\n\n\n\nMethod 2: Process with SQL in BigQuery\nMethod 2 leverages the power of SQL within BigQuery to perform all data processing, including filtering, joining, and birth weight calculations, directly in the SQL query within BigQuery itself. This approach minimizes the need for additional R data manipulation.\nLet’s create an SQL query in BigQuery that handles all the required data transformations:\n# Build the SQL query using glue to perform all data processing in BigQuery\nsql_query &lt;- glue::glue(\n  \"SELECT\n    t1.*,\n    t2.*,\n    weight_pounds * 0.453592 AS birth_weight_kg\n  FROM\n    `{project_id}.{dataset}.{table}` AS t1\n  JOIN\n    `{project_id}.{dataset}.{table}` AS t2\n  ON\n    t1.source = t2.source\n  WHERE\n    t1.is_male = 1\"\n)\n\n# Execute the query\nquery_result &lt;- bigrquery::bq_query(sql_query)\n\n# Fetch the processed data directly from BigQuery\nresult &lt;- bigrquery::bq_table_download(query_result)\nPros\n\nSQL Expertise: Ideal for users with SQL expertise, as data filtering, joining, and transformation can be done entirely in SQL, leveraging the power of BigQuery.\nEfficiency: SQL queries can be optimized for efficient data retrieval and processing within BigQuery.\n\nCons\n\nSQL Knowledge Required: Requires proficiency in SQL for writing complex queries, which may not be suitable for all R users.\nPotential for Complex SQL: Complex SQL queries can become difficult to manage and debug.\n\n\n\nMethod 3: Using dbConnect and dbplyr\nIn Method 3, we establish a database connection and leverage dbplyr to perform data filtering, joins, and transformations directly in BigQuery. This method combines SQL and R flexibility while utilizing lazy evaluation.\nmethod3_function &lt;- function(project_id, dataset, table) {\n  # Establish a database connection\n  con &lt;- DBI::dbConnect(bigrquery::bigquery(), project = project_id, dataset = dataset)\n  \n  # Query and process the data using dbplyr\n  data &lt;- dbplyr::tbl(con, table) %&gt;%\n    dplyr::filter(is_male == 1) %&gt;%\n    dplyr::inner_join(dbplyr::tbl(con, table), by = \"source\") %&gt;%\n    dplyr::mutate(birth_weight_kg = weight_pounds * 0.453592)\n  \n  # Disconnect from the database\n  DBI::dbDisconnect(con)\n  return(data)\n}\n\ndf3 &lt;- method2_function(project_id, dataset, table)\nhead(df3)\nPros\n\nLazy Evaluation: Utilizes lazy evaluation, allowing you to build and optimize the query step by step, enhancing query efficiency.\nSQL and R Integration: Offers the benefits of both SQL and R, making it flexible for users with varying levels of SQL expertise.\n\nCons\n\nDatabase Connection Overhead: Establishing and managing a database connection can add some overhead to the process.\nLearning Curve: Users not familiar with dbplyr may need time to learn and adapt to this method.\n\n\n\nMethod 4: Using “Materialized View” in BigQuery\nMethod 4 involves creating a materialized view in BigQuery that includes all data filtering, transformations, and joins. We access preprocessed data directly from the view, reducing the need for complex transformations in R.\nCreate a Materialized View in BigQuery:\nCREATE MATERIALIZED VIEW `your_project.your_dataset.your_materialized_view` AS\nSELECT\n  column1,\n  column2,\n  SUM(sales_amount) AS total_sales,\n  weight_pounds * 0.453592 AS birth_weight_kg\nFROM\n  `your_project.your_dataset.sales_data`\nWHERE\n  is_male = 1\nGROUP BY\n  column1, column2, birth_weight_kg;\nAccessing Data from the Materialized View\nNow, let’s load and process data using Method 4, where all data analysis is performed within the SQL query that generates the materialized view. We won’t need additional R data manipulation in this method.\nmethod4_function &lt;- function(project_id, dataset, table) {\n  # Build the SQL query to directly access the materialized view\n  sql_query &lt;- glue::glue(\"SELECT * FROM `{project_id}.{dataset}.{table}_materialized_view`\")\n  \n  # Execute the query\n  query_result &lt;- bigrquery::bq_query(sql_query)\n  \n  # Fetch the data directly from the materialized view\n  result &lt;- bigrquery::bq_table_download(query_result)\n  \n  return(result)\n}\n\ndf4 &lt;- method4_function(project_id, dataset, table)\nhead(df4)\nPros\n\nPreprocessed Data: Materialized views in BigQuery store preprocessed and aggregated data, reducing the need for complex transformations in R.\nEfficiency: Retrieving data from a materialized view is often faster than processing raw data.\n\nCons\n\nLimited Flexibility: Materialized views are predefined, limiting flexibility for custom transformations or ad-hoc queries.\nMaintenance: Materialized views require maintenance to ensure they stay up-to-date with source data changes."
  },
  {
    "objectID": "optimizing-data-handling.html#verification",
    "href": "optimizing-data-handling.html#verification",
    "title": "Optimizing Data Handling with R and BigQuery",
    "section": "Verification",
    "text": "Verification\nTo ensure the correctness of our methods, we’ll verify that the data frames returned by each method are identical.\n# Verify data frame equality\nidentical_df1_df2 &lt;- identical(df1, df2)\nidentical_df1_df3 &lt;- identical(df1, df3)\nidentical_df1_df4 &lt;- identical(df1, df4)\nidentical_df2_df3 &lt;- identical(df2, df3)\nidentical_df2_df4 &lt;- identical(df2, df4)\nidentical_df3_df4 &lt;- identical(df3, df4)\n\n# Display data frame equality verification\ncat(\"Data Frame Equality Verification:\\n\")\ncat(\"Method 1 and Method 2: \", identical_df1_df2, \"\\n\")\ncat(\"Method 1 and Method 3: \", identical_df1_df3, \"\\n\")\ncat(\"Method 1 and Method 4: \", identical_df1_df4, \"\\n\")\ncat(\"Method 2 and Method 3: \", identical_df2_df3, \"\\n\")\ncat(\"Method 2 and Method 4: \", identical_df2_df4, \"\\n\")\ncat(\"Method 3 and Method 4: \", identical_df3_df4, \"\\n\")"
  },
  {
    "objectID": "optimizing-data-handling.html#benchmarking",
    "href": "optimizing-data-handling.html#benchmarking",
    "title": "Optimizing Data Handling with R and BigQuery",
    "section": "Benchmarking",
    "text": "Benchmarking\nTo determine which method is most efficient, let’s benchmark each approach.\nlibrary(microbenchmark)\n\n# Benchmark each method\nbenchmark_results &lt;- microbenchmark(\n  Method1 = method1_function(project_id, dataset, table),\n  Method2 = method2_function(project_id, dataset, table),\n  Method3 = method3_function(project_id, dataset, table),\n  Method4 = method4_function(project_id, dataset, table),\n  times = 100\n)\n\n# Compare and summarize the benchmark results\nsummary(benchmark_results)"
  },
  {
    "objectID": "optimizing-data-handling.html#conclusions",
    "href": "optimizing-data-handling.html#conclusions",
    "title": "Optimizing Data Handling with R and BigQuery",
    "section": "Conclusions",
    "text": "Conclusions\nIn conclusion, the choice of data handling method in BigQuery and R depends on your specific needs. If simplicity and familiarity are your priorities, Method 1 may be suitable. For those proficient in SQL, Method 2 provides efficiency. Method 3 combines SQL and R flexibility, while Method 4 excels in efficiency and preprocessed data. By understanding these methods, you can optimize your data workflows and make informed decisions, improving your data analysis projects."
  },
  {
    "objectID": "optimizing-data-handling.html#references-and-helpful-links",
    "href": "optimizing-data-handling.html#references-and-helpful-links",
    "title": "Optimizing Data Handling with R and BigQuery",
    "section": "References and Helpful Links",
    "text": "References and Helpful Links\n\nUsing dbConnect and dbplyr:\n\nIntroduction to dbplyr\ndbplyr SQL translation\nSQL queries in RMD\n\nQuerying BigQuery using SQL directly in rmarkdown:\n\nRMD and SQL"
  },
  {
    "objectID": "contributing-to-github.html",
    "href": "contributing-to-github.html",
    "title": "GitHub Contributions Tutorial",
    "section": "",
    "text": "Summary:\nThis guide will help you clone a feature branch from a GitHub repo, keep it up-to-date with the main branch, make well-documented commits, and create a pull request. We’ll emphasize explicit git add commands, ignoring unnecessary files, and linking to GitHub issues for trace-ability.\n\n\n1. Clone the Feature Branch:\n\nOpen your terminal.\nNavigate to the directory where you want to clone the project.\nRun the following command to clone the feature branch:\ngit clone -b feature-branch https://github.com/Analyticsphere/featureBranch.git\nReplace feature-branch with your feature branch name. Replace https://github.com/Analyticsphere/featureBranch.git with the URL for the specific branch that you are cloning.\n\n\n\n2. Keep Your Local Repository Up-to-Date:\n\nEnsure you’re on your feature branch:\ngit checkout feature-branch\nFetch the latest changes from the main branch:\ngit fetch origin main\nMerge the main branch into your feature branch:\ngit merge origin/main\n\n\n\n3. Make Explicit Commits:\n\nMake changes to your code.\nStage your changes explicitly, one by one:\ngit add file1.R\ngit add file2.py\nEnter git status to view all staged/upstaged files. Add only the files that you’d like to track.\nCommit your changes with a meaningful message and Link to GitHub issues for trace-ability.:\ngit commit -m \"Fix issue #X: Description of the change.\"\nEnsure you follow best practices in commit messages.\n\n\n\n4. Ignoring Unnecessary Files:\n\nOpen the .gitignore file in your project.\nAdd the following lines to ignore .Rhistory, .Rdata, and .Rproj files: .Rhistory .Rdata .Rproj.user/\nSave and commit the .gitignore changes.\n\n\n\n5. Push Changes to GitHub:\n\nPush your changes to your feature branch:\ngit push origin feature-branch\n\n\n\n6. Create a Pull Request:\n\nVisit your GitHub repository.\nClick “New Pull Request.”\nSet the base branch to main and the compare branch to your feature-branch.\nProvide a clear title and description for the pull request.\nSubmit the pull request for code review.\n\n\n\n7. Delete the feature branch:\n\nIf your branch is related to a specific feature that now merged with the main branch, it is a good practice to delete it, both locally and in the GitHub repo.\nWhen you work on a new feature, create a new branch and begin again!"
  },
  {
    "objectID": "team-roles.html",
    "href": "team-roles.html",
    "title": "Team Roles",
    "section": "",
    "text": "Current Members\n\nNicole, Lead Analyst:\n\nThe boss\n\nJing\n\n\n\nKelsey\n\n\n\nJake Peters, Data Scientist:\n\nAutomate/maintain reporting pipelines in GCP\nDevelop/maintain flattening workflow for BigQuery tables\nAutomate QAQC workflow\nDevelop/maintain connection from GCP to Box\n\nMadhuri\n\n\n\nRebecca\n\n\n\n\n\n\nPast Members"
  },
  {
    "objectID": "bq2.html",
    "href": "bq2.html",
    "title": "BQ2 Work Flow",
    "section": "",
    "text": "This diagram depicts the high-level work flow for generating the BQ2 data.\n\n[Lucid Chart Link]\nKey Points:\n\nAll arrows indicate scheduled data transfers or SQL queries.\nBQ2 will contain only de-identified data\nThe main BQ2 dataset will have the original Concept IDs from the Data Dictionary (except for derived variables like binned age)\nAll flattening, merging, cleaning and de-identification will occur in BQ1\nDerived materialized views will be used for the Stakeholder Metrics Dashboard (and other similar reports). They will reference the Main BQ2 data set, not the BQ1 data set. This will respect the PII barrier.\n\nWe can discuss this further as a group! Just wanted to jot this down so that we have a common reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Analytics Team Wiki",
    "section": "",
    "text": "Why a wiki?\nThis wiki is intended to gather the wealth of institutional knowledge that we have developed on this team into a persistent resource. This is especially useful for on-boarding new members, training colleagues to cover us while on PTO and for developing some overlapping knowledge across roles. We can also use this wiki to develop shared resources and formalize best practices that we would like to follow on our team as we grow and our work becomes more complex.\n\n\nWho should contribute?\nEveryone!\n\nIf you learn a new skill that could be useful to others, make a minimal tutorial and add it to the wiki!\nIf you create something new that will be part of the project for a while, add a documentation page to the wiki!\nIf you come across a resource that could be valuable to others, add a link to the wiki!\n\n\n\nHow to contribute\nYou can contribute a new page to the wiki in the form of an RMD file, QMD file, an MD file or an ipynb file. They can be easily added to the appropriate section using the _quarto.yml file, which ultimately configures how the site is rendered.\nThe code for this website live in this repo: https://github.com/Analyticsphere/analytics-team-wiki\nPushing commits to the main branch automatically updates the website via GitHub Pages. The site must be rendered locally using quarto render prior to pushing changes.\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\nRules\n\nNever share PII or PHI on this site!\nNever share secret tokens or passwords.\nDo not share links to Box or other resources that do not require authentication.\nWhen in doubt, ask Jake."
  }
]